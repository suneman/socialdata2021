{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The intro\n",
    "\n",
    "Anyway. I'm sure you guys have a lot to do this week, so we'll try to keep it relatively light (although there should be enough optional exercises to keep you all busy).\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/6b4EQk96SfQ/0.jpg)](https://www.youtube.com/watch?v=6b4EQk96SfQ)\n",
    "\n",
    "Remember that last week you worked classifing data using *KNN*'s. We are going to continue working with machine learning, this time looking at *decision trees* and see how new information can influence the performance of our model in predicting which type of crime happened.\n",
    "\n",
    "Specifically, crimes can have many causes, so we can combine datasouces to better understand what makes a criminal commit a crime. Are there specific factors which trigger that individual to act? Since criminals are notoriously shy about sharing information, we must try to find this out in a different way. Lucky for us, we can do this with data! \n",
    "\n",
    "*We are going to use weather data* from San Franciso to try to relate different crimes with meteorological conditions!\n",
    "\n",
    "* We'll start with a relatively simple exercise focusing adding weather data to the decision tree from last week (Part 1, 2, and 3).\n",
    "* Then we'll prepare a bit for next week, when we get into the topic of explanatory data visualization with some lectures and reading (Part 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Decision Tree Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn to decision trees. This is a fantastically useful supervised machine-learning method, that we use all the time in research. To get started on the decision trees, we'll use some fantastic *visual* introduction. \n",
    "\n",
    "\n",
    "*Decision Trees Reading 1*: The visual introduction to decision trees on this webpage is AMAZING. Take a look to get an intuitive feel for how trees work. Do not miss this one, it's a treat! http://www.r2d3.us/visual-intro-to-machine-learning-part-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Decision Trees Reading 2*: the second part of the visual introduction is about the topic of model selection, and bias/variance tradeoffs that we looked into earlier during this lesson. But once again, here those topics are visualized in a fantastic and inspiring way, that will make it stick in your brain better. So check it out http://www.r2d3.us/visual-intro-to-machine-learning-part-2/\n",
    "\n",
    "*Decision Trees Reading 3*: Finally, you can also read about decision trees in DSFS, chapter 17. **You can get it on DTU Learn**\n",
    "\n",
    "And our little session on decision trees wouldn't be complete without hearing from Ole about these things. \n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/LAA_CnkAEx8/0.jpg)](https://www.youtube.com/watch?v=LAA_CnkAEx8)\n",
    "\n",
    "\n",
    "*Decision tree \"reading\" 4*: And of course the best way to learn how to get this stuff rolling in practice, is to work through a tutorial or two. We recommend the ones below:\n",
    "  * https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n",
    "  * https://towardsdatascience.com/random-forest-in-python-24d0893d51c0 (this one also has good considerations regarding the one-hot encodings)\n",
    "  \n",
    "(But there are many other good ones out there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ole explains decision trees\n",
    "# YouTubeVideo(\"LAA_CnkAEx8\",width=600, height=338)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Exercises: Just a few questions to make sure you've read the text (DSFS chapter 17) and/or watched the video.\n",
    "> \n",
    "> * There are two main kinds of decision trees depending on the type of output (numeric vs. categorical). What are they?\n",
    "> * Explain in your own words: Why is entropy useful when deciding where to split the data?\n",
    "> * Why are trees prone to overfitting?\n",
    "> * Explain (in your own words) how random forests help prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Decision Tree Baseline\n",
    "\n",
    "\n",
    "> *Exercise*: Decision trees and real-world crime data\n",
    "> \n",
    "> The idea for today is to pick two crime-types that have *different geographical patterns* and *different temporal patterns*. We can then use various variables of the real crime data as categories to build a decision tree. I'm thinking we can use\n",
    "> * `DayOfWeek` (`Sunday`, ..., `Saturday`). (Note: Will need to be encodede as integer in `sklearn`)\n",
    "> * `PD District` (`TENDERLOIN`, etc). (Note: Will need to be encodede as integer in `sklearn`)\n",
    "> \n",
    "> And we can extract a few more from the `Time` and `Date` variables\n",
    "> * Hour of the day (1-24)\n",
    "> * Month of the year (1-12)\n",
    "> \n",
    "> So your job is to **select two crime categories** that (based on your analyses from the past three weeks) have different spatio-temporal patterns. Since we will use weather data to disitinguish later, let's try to think of crime categories that our intuition tells us might be strongly influenced by the weather conditions (type 1). And also think of other categories where we **don't** expect weather to play a role (type 2). We suggest:\n",
    "\n",
    "* `BURGLARY or VEHICLE THEFT` for type 1. \n",
    "* `FORGERY/COUNTERFEITIN or FRAUD` for type 2. \n",
    "\n",
    "But you are free to choose other ones, if you like ðŸ¤“\n",
    "\n",
    "Now we are going to to build is a decision tree (or, even better, a [Random Forest](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html), here is [another tutorial for Random Forests](https://towardsdatascience.com/random-forest-in-python-24d0893d51c0)) classifier that takes as input the four labels (Hour-of-the-day, Day-of-the-week, Month-of-the-year, and PD-District) of a crime (from one of the two categories) and then tries to predict which category that crime is from.\n",
    ">\n",
    "> Some notes/hints\n",
    "> * Remember to create a balanced dataset, that is, **grab an equal number of examples** from each of the two crime categories. Pick categories with lots of training data. It's probably nice to have something like 10000+ examples of each category to train on. \n",
    "> * Also, I recommend you grab your training data at `random` from the set of all examples, since we want crimes to be distributed equally over time.\n",
    "> * A good option is the  `DecisionTreeClassifier`.\n",
    "> * We recommed you build a separate Pandas `Dataframe` with it, so the process of adding the weather data will be as smooth as possible later on. The same goes for your testing data.\n",
    "> * Create a function to evaluate the precision of your classifier. Make sure your test data is not used for training. (Since you have created a balanced dataset, the baseline performance (random guess) is 50%. How good can your classifier get?)\n",
    "> * (Optional, although this one might improve performance). Does one hot encoding affect your results? Why/Why not?  \n",
    "> * (Optional) Are your results tied to the specific training data you used? Are you overfitting? Try performing [cross-validation](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f) to answer this question.\n",
    "> * (Optional) If you find yourself with extra time, come back to this exercise and tweak the variables you use to see if you can improve the accuracy of the tree. Try for example adding Year, Month or other variables you think may be relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Beyond the Baseline with Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 2, you built a Decision Tree/Random Forest classifier to predict the category of a crime with the help of our friend `sklearn` using the following variables:\n",
    "\n",
    "* `Hour of the week` (`1 , 2, ..., 168 `). \n",
    "* `PD District` (`TENDERLOIN`, etc).  (**Remember**, You'll need to encode this  labels as integers in `sklearn`, you can just assign numbers to the labels with something like sklearn's `Label Encoder` or do your own custom function). \n",
    "\n",
    "That model from Part 2 will function as our baseline. Now that we have it set up, we can use it to understand how adding variables from a **weather dataset** will influence the decisions of the tree later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get that weather data rolling. The raw data we are using can be found online [here](https://www.meteoblue.com/en/weather/archive/export/san-francisco_united-states-of-america_5391959) or [you can get a convenient version from the files folder our class repository by clicking here](https://raw.githubusercontent.com/suneman/socialdata2021/master/files/weather_data.csv). \n",
    "\n",
    "> *Exercise*\n",
    "> \n",
    "> * Load the weather dataset. If you have your training data and test data on separate `DataFrames` then merging them with the weather information should be simple \n",
    ">   * **Hint**: you can use the join method from pandas. To do so, you will need to round the time to the hour because weather data is recorded hourly. Also it's fine to drop missing values. Here's a [stackoverflow post](https://stackoverflow.com/questions/36292959/pandas-merge-data-frames-on-datetime-index) which may help you. \n",
    ">  * *Note*: you'll need to do some encoding on the weather data as before if you want to use the weather column. Also, check if all of the entries of the new training data have indeed a weather part to them. \n",
    "> * Now that you have the data properly merged, you can **fit a new random forest on the data and compare the results**. How does the weather data influence the prediction performances? (Use the evaluation function you built above.) Is there as impact in the accuracy of predictions? Is weather data relevant for the predictions?\n",
    "> * *Optional*: Try experimenting with using only certain variables of the weather data. Can you improve the performance of classification by using fewer features/variables?\n",
    "\n",
    "\n",
    "**Note**: It's not 100% given that adding weather will improve your predictive performance. It can go either way depending on the details of your implementation. The important thing is not performance, but that you implement your code in the right way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Video Lectures and Reading\n",
    "\n",
    "Next week we'll be playing around with *explanatory data visualization*. Roughly speaking this means using data visualization to communicate your results to others. Thus, there are new things to think about. We'll start thinking about that already this week.\n",
    "\n",
    "We start with a video from from yours truly and then read a bit from a scientific article about types of explanatory dataviz. (*The video is from an old version of the class that used D3, so just ignore those parts. I'll make a new one ASAP*).\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/yHKYMGwefso/0.jpg)](https://www.youtube.com/watch?v=yHKYMGwefso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sune talks about designing visualizations.\n",
    "# from IPython.display import YouTubeVideo\n",
    "# YouTubeVideo(\"yHKYMGwefso\",width=600, height=338)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercises*: Explanatory data visualization\n",
    "> * What are the three key elements to keep in mind when you design an explanatory visualization?\n",
    "> * In the video I talk about (1) *overview first*,  (2) *zoom and filter*,  (3) *details on demand*. \n",
    ">   - Go online and find a visualization that follows these principles (don't use one from the video). \n",
    ">   - Explain how it does achieves (1)-(3). It might be useful to use screenshots to illustrate your explanation.\n",
    "> * Explain in your own words: How is explanatory data analysis different from exploratory data analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reading*: [Narrative Visualization: Telling Stories with Data](http://vis.stanford.edu/files/2010-Narrative-InfoVis.pdf) by Edward Segel and Jeffrey Heer. We'll read section 1-3 today. (And the rest next time).\n",
    "\n",
    "When you get to section 3 it's fun to open up the examples mentioned by the authors in a browser and explore them as you read the text. \n",
    "\n",
    "> *Exercise*: Answer a couple of questions about the paper.\n",
    "> \n",
    "> * What is the *Oxford English Dictionary's* defintion of a narrative?\n",
    "> * What is your favorite visualization among the examples in section 3? Explain why in a few words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
